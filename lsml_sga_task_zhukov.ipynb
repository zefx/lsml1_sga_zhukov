{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User routes on the site\n",
    "## Description\n",
    "**Clickstream** is a sequence of user actions on a website. It allows you to understand how users interact with the site. In this task, you need to find the most frequent custom routes.\n",
    "\n",
    "## Input data\n",
    "Input data is а table with clickstream data in file `hdfs:/data/clickstream.csv`.\n",
    "\n",
    "### Table structure\n",
    "* `user_id (int)` - Unique user identifier.\n",
    "* `session_id (int)` - Unique identifier for the user session. The user's session lasts until the identifier changes.\n",
    "* `event_type (string)` - Event type from the list:\n",
    "    * **page** - visit to the page\n",
    "    * **event** - any action on the page\n",
    "    * <b>&lt;custom&gt;</b> - string with any other type\n",
    "* `event_type (string)` - Page on the site.\n",
    "* `timestamp (int)` - Unix-timestamp of action.\n",
    "\n",
    "### Browser errors\n",
    "Errors can sometimes occur in the user's browser - after such an error appears, we can no longer trust the data of this session and all the following lines after the error or at the same time with it are considered corrupted and **should not be counted** in statistics.\n",
    "\n",
    "When an error occurs on the page, a random string containing the word **error** will be written to the `event_type` field.\n",
    "\n",
    "### Sample of user session\n",
    "<pre>\n",
    "+-------+----------+------------+----------+----------+\n",
    "|user_id|session_id|  event_type|event_page| timestamp|\n",
    "+-------+----------+------------+----------+----------+\n",
    "|    562|       507|        page|      main|1620494781|\n",
    "|    562|       507|       event|      main|1620494788|\n",
    "|    562|       507|       event|      main|1620494798|\n",
    "|    562|       507|        page|    family|1620494820|\n",
    "|    562|       507|       event|    family|1620494828|\n",
    "|    562|       507|        page|      main|1620494848|\n",
    "|    562|       507|wNaxLlerrorU|      main|1620494865|\n",
    "|    562|       507|       event|      main|1620494873|\n",
    "|    562|       507|        page|      news|1620494875|\n",
    "|    562|       507|        page|   tariffs|1620494876|\n",
    "|    562|       507|       event|   tariffs|1620494884|\n",
    "|    562|       514|        page|      main|1620728918|\n",
    "|    562|       514|       event|      main|1620729174|\n",
    "|    562|       514|        page|   archive|1620729674|\n",
    "|    562|       514|        page|     bonus|1620729797|\n",
    "|    562|       514|        page|   tariffs|1620731090|\n",
    "|    562|       514|       event|   tariffs|1620731187|\n",
    "+-------+----------+------------+----------+----------+\n",
    "</pre>\n",
    "\n",
    "#### Correct user routes for a given user:\n",
    "* **Session 507**: main-family-main\n",
    "* **Session 514**: main-archive-bonus-tariffs\n",
    "\n",
    "Route elements are ordered by the time they appear in the clickstream, from earliest to latest.\n",
    "\n",
    "The route must be accounted for completely before the end of the session or an error in the session.\n",
    "\n",
    "## Task\n",
    "You need to use the Spark SQL, Spark RDD and Spark DF interfaces to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site.\n",
    "\n",
    "Each line of the file should contain the `route` and `count` values **separated by tabs**, where:\n",
    "* `route` - route on the site, consisting of pages separated by \"-\".\n",
    "* `count` - the number of user sessions in which this route was.\n",
    "\n",
    "The lines must be **ordered in descending order** of the `count` field.\n",
    "\n",
    "## Criteria\n",
    "You can get maximum of 3.5 points (final grade) for this assignment, depedning on the number of interface you manage to leverage. The criteria are as follows:\n",
    "\n",
    "* 0.5 points – Spark SQL solution with 1 query\n",
    "* 0.5 points – Spark SQL solution with <=2 queries\n",
    "* 0.5 points – Spark RDD solution\n",
    "* 0.5 points – Spark DF solution\n",
    "* 0.5 points – your solution algorithm is relatively optimized, i.e.: no O^2 or O^3 complexities; appropriate object usage; no data leaks etc. This is evaluated by staff.\n",
    "* 1 point – 1 on 1 screening session. During this session staff member can ask you questions regarding your solution logic, framework usage, questionable parts of your code etc. If your code is clean enough, the staff member can just ask you to solve a theoretical problem connected to Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-10-08 18:02:12,323 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "\n",
    "sc = pyspark.SparkContext(appName='jupyter')\n",
    "se = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !hadoop fs -put clickstream.csv clickstream.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+----------+----------+\n",
      "|user_id|session_id|  event_type|event_page| timestamp|\n",
      "+-------+----------+------------+----------+----------+\n",
      "|    562|       507|        page|      main|1695584127|\n",
      "|    562|       507|       event|      main|1695584134|\n",
      "|    562|       507|       event|      main|1695584144|\n",
      "|    562|       507|       event|      main|1695584147|\n",
      "|    562|       507|wNaxLlerrorU|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584154|\n",
      "|    562|       507|       event|      main|1695584160|\n",
      "|    562|       507|        page|    rabota|1695584166|\n",
      "|    562|       507|       event|    rabota|1695584174|\n",
      "+-------+----------+------------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading csv\n",
    "c_df = se.read.csv(\"/data/clickstream.csv\", sep='\\t', header=True, inferSchema=True)\n",
    "c_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !hadoop fs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating temp table \"stream\" in spark sql catalog\n",
    "c_df.orderBy(\"user_id\", \"session_id\", \"timestamp\")\n",
    "c_df.createOrReplaceTempView(\"stream_init\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20448"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking distinct values for event_type just to understand if they can be excluded easily\n",
    "c_df.select('event_type').distinct().count()\n",
    "# nope, we have many different errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Spark SQL solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### As I see in the tesk example - we count only event_type 'page' in our route. I will treat all routes for event_type='page' until error or end of session, as no additional restrictions are definied in the task itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# se.sql('SHOW COLUMNS FROM stream').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- session_id: integer (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_page: string (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I'd tried approach not similar to the logic we used for DF solution, but something is missing as I getting slightly lower numbers - had no time to find the reason - not very convenient debug SQL wih HDFS :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sql_rts_count = se.sql(\"\"\"\n",
    "# WITH corr_sessions_err AS \n",
    "#     (SELECT * FROM stream_df WHERE event_type = 'page' AND stream_df.timestamp < (SELECT MIN(slave.timestamp) \n",
    "#     FROM stream_df AS slave WHERE slave.user_id = stream_df.user_id AND slave.event_type LIKE '%error%')),        \n",
    "# corr_sessions_no_err AS \n",
    "#     (SELECT * FROM stream_df s WHERE s.event_type = 'page' AND NOT EXISTS\n",
    "#         (SELECT 1 FROM corr_sessions_err c WHERE (s.user_id = c.user_id AND s.session_id = c.session_id))\n",
    "#     ),\n",
    "# res_tbl AS \n",
    "#     ((SELECT * FROM corr_sessions_err UNION ALL SELECT * FROM corr_sessions_no_err)\n",
    "#     ORDER BY user_id, session_id, timestamp ASC),\n",
    "# res_routes AS \n",
    "#     (SELECT user_id, session_id, COLLECT_LIST(event_page) AS rt FROM res_tbl GROUP BY user_id, session_id)\n",
    "#         (SELECT CONCAT_WS('-', rt) as route, COUNT(*) as count\n",
    "#         FROM res_routes\n",
    "#         GROUP BY route\n",
    "#         ORDER BY count DESC\n",
    "#         LIMIT 30)\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second approach I did after DF was done, so logic is ismilar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql_rts_count = se.sql(\"\"\"\n",
    "WITH corr_sessions_err AS \n",
    "    (SELECT user_id, session_id, MIN(timestamp) AS err_t FROM stream_init WHERE event_type LIKE '%error%'\n",
    "    GROUP BY user_id, session_id),\n",
    "res_tbl AS \n",
    "    (SELECT s.user_id, s.session_id, s.event_page, s.timestamp FROM stream_init s\n",
    "    LEFT JOIN corr_sessions_err c ON s.user_id = c.user_id AND s.session_id = c.session_id\n",
    "    WHERE s.event_type = 'page' AND (s.timestamp < c.err_t OR c.err_t IS NULL)),\n",
    "res_routes AS \n",
    "    (SELECT user_id, session_id, COLLECT_LIST(event_page) AS rt FROM res_tbl GROUP BY user_id, session_id)\n",
    "        (SELECT CONCAT_WS('-', rt) AS route, COUNT(*) AS count\n",
    "        FROM res_routes\n",
    "        GROUP BY route\n",
    "        ORDER BY count DESC\n",
    "        LIMIT 30)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|        route|count|\n",
      "+-------------+-----+\n",
      "|         main| 8090|\n",
      "| main-archive| 1091|\n",
      "|  main-rabota| 1035|\n",
      "|main-internet|  880|\n",
      "|   main-bonus|  864|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_rts_count.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# uploading data for 30 entries\n",
    "sql_rts_count.write.csv(\"hdfs:/data/lsml_sga_sql_zhukov.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SQL with 2 queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be easily done by separating 1-query solution into 2, so sorry do not see any purpose to use the same query here separated in two :) It will not help with complexity or speed as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spark RDD solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define RDD to work with from our external file\n",
    "stream = sc.textFile(\"/data/clickstream.csv\").cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user_id\\tsession_id\\tevent_type\\tevent_page\\ttimestamp'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting column names as 1st line\n",
    "column_names = stream.first()\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filtering stream only for stream data + split by \\t\n",
    "stream = stream.filter(lambda line: line != column_names).map(lambda line:line.split('\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['562', '507', 'page', 'main', '1695584127'],\n",
       " ['562', '507', 'event', 'main', '1695584134'],\n",
       " ['562', '507', 'event', 'main', '1695584144'],\n",
       " ['562', '507', 'event', 'main', '1695584147'],\n",
       " ['562', '507', 'wNaxLlerrorU', 'main', '1695584154']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking values of first strings\n",
    "stream.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# creating keys from user id and session id and aggregate by them sorted by timestamp\n",
    "step_1 = stream.map(lambda row: ((row[0], row[1]), [row[2], row[3], row[4]])).reduceByKey(lambda a, b: a + b).sortBy(lambda x: x[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def err(elem):\n",
    "    lst = []\n",
    "    for i in elem:\n",
    "        if 'error' not in i:\n",
    "            lst.append(i)\n",
    "        else:\n",
    "            return lst\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning only stream until error - user id and and session id \n",
    "# not needed anymore as we have reduced by them as keys previously\n",
    "step_2 = step_1.map(lambda x: err(x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['page',\n",
       "  'main',\n",
       "  '1695584127',\n",
       "  'event',\n",
       "  'main',\n",
       "  '1695584134',\n",
       "  'event',\n",
       "  'main',\n",
       "  '1695584144',\n",
       "  'event',\n",
       "  'main',\n",
       "  '1695584147']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_2.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def del_elem(lst, place_n):\n",
    "    del lst[place_n-1::place_n]\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# deleting all timestamps as we have arranged lists\n",
    "step_3 = step_2.map(lambda x: del_elem(x, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['page', 'main', 'event', 'main', 'event', 'main', 'event', 'main']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_3.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def only_pages(lst):\n",
    "    empty = []\n",
    "    for i in range(0, len(lst),2):\n",
    "        if lst[i] == 'page':\n",
    "            empty.append(lst[i+1])\n",
    "    return empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['main']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_pages(['page', 'main', 'event', 'main', 'event', 'main', 'event', 'main'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# selecting only if event in stream is \"page\" type\n",
    "step_4 = step_3.map(lambda x: only_pages(x)).map(lambda x: (\"-\".join(x), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('main', 1),\n",
       " ('main-online-bonus-news-main-news-vklad-rabota-bonus-tariffs', 1),\n",
       " ('main-internet', 1),\n",
       " ('main', 1),\n",
       " ('main', 1),\n",
       " ('main-tariffs-main-online-news-main-internet-online-news-main-news', 1),\n",
       " ('main-archive-news-tariffs-main-rabota-main-online', 1),\n",
       " ('main', 1),\n",
       " ('main-bonus-main', 1),\n",
       " ('main-archive-internet-archive-main-tariffs-main-vklad', 1)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_4.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count = step_4.reduceByKey(lambda a, b: a + b).sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('main', 8091),\n",
       " ('main-archive', 1096),\n",
       " ('main-rabota', 1039),\n",
       " ('main-internet', 880),\n",
       " ('main-bonus', 865)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking\n",
    "count.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# uploading data for 30 entries\n",
    "count_rdd_df = count.toDF().orderBy(F.desc(\"_2\")).limit(30).write.csv(\"hdfs:/data/lsml_sga_rdd_zhukov.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spark DF Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# csv to Spark DF\n",
    "df = se.read.csv(\"hdfs:/data/clickstream.csv\", header=True, sep=\"\\t\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df = df.orderBy(['user_id', 'session_id', 'timestamp'], ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create window func over user id and session id\n",
    "window = Window.partitionBy(*[\"user_id\", \"session_id\"]).orderBy(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "err_tst = (df.filter(F.col(\"event_type\").like(\"%error%\")).groupBy(\"user_id\", \"session_id\").agg(F.min(\"timestamp\").alias(\"err_ts\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+\n",
      "|user_id|session_id|    err_ts|\n",
      "+-------+----------+----------+\n",
      "|      0|       879|1696777359|\n",
      "|      0|       898|1697629831|\n",
      "|      0|       901|1699003239|\n",
      "|      0|       912|1700596574|\n",
      "|      0|       922|1700724457|\n",
      "+-------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking\n",
    "err_tst.orderBy(['user_id', 'session_id', 'err_ts'], ascending=True).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# joining 2 tables on user id and session id + sorted\n",
    "joined_tbls = (df.join(err_tst,on=[\"user_id\", \"session_id\"],how=\"left_outer\")).orderBy([\"user_id\", \"session_id\", \"timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checking\n",
    "# joined_tbls.show(100)\n",
    "# checks ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filtering to get event_type only 'page' and remove all aftere first error in a session \n",
    "fltrd_joined_tbls = joined_tbls.filter((F.col(\"event_type\") == \"page\") & ((F.col(\"timestamp\") < F.col(\"err_ts\")) | F.col(\"err_ts\").isNull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:==============>                                           (2 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+----------+----------+----------+\n",
      "|user_id|session_id|event_type|event_page| timestamp|    err_ts|\n",
      "+-------+----------+----------+----------+----------+----------+\n",
      "|      0|       874|      page|      main|1696371064|      null|\n",
      "|      0|       874|      page|    rabota|1696374894|      null|\n",
      "|      0|       874|      page|    online|1696378229|      null|\n",
      "|      0|       879|      page|      main|1696768416|1696777359|\n",
      "|      0|       879|      page|    online|1696768738|1696777359|\n",
      "|      0|       879|      page|   tariffs|1696768973|1696777359|\n",
      "|      0|       879|      page|    online|1696769277|1696777359|\n",
      "|      0|       879|      page|      main|1696773185|1696777359|\n",
      "|      0|       879|      page|  internet|1696774086|1696777359|\n",
      "|      0|       879|      page|    online|1696776502|1696777359|\n",
      "|      0|       885|      page|      main|1697348270|      null|\n",
      "|      0|       888|      page|      main|1697564550|      null|\n",
      "|      0|       898|      page|      main|1697594437|1697629831|\n",
      "|      0|       898|      page|      news|1697596341|1697629831|\n",
      "|      0|       898|      page|   tariffs|1697598240|1697629831|\n",
      "|      0|       898|      page|    rabota|1697600131|1697629831|\n",
      "|      0|       898|      page|     bonus|1697605450|1697629831|\n",
      "|      0|       898|      page|   tariffs|1697611362|1697629831|\n",
      "|      0|       898|      page|     bonus|1697612492|1697629831|\n",
      "|      0|       898|      page|  internet|1697614807|1697629831|\n",
      "+-------+----------+----------+----------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fltrd_joined_tbls.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# aggregating max length routes into separate column over defined window (by user and session)\n",
    "rts = (fltrd_joined_tbls.withColumn(\"rts\", F.collect_list(\"event_page\").over(window)).groupBy(\"user_id\", \"session_id\")).agg(F.max(\"rts\").alias(\"route\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+\n",
      "|user_id|session_id|               route|\n",
      "+-------+----------+--------------------+\n",
      "|      0|       874|[main, rabota, on...|\n",
      "|      0|       898|[main, news, tari...|\n",
      "|      0|       901|[main, internet, ...|\n",
      "+-------+----------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rts.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#concat routes to correct format and count routes ordered desc\n",
    "rts_fin = rts.withColumn(\"route\", F.expr(\"concat_ws('-', route)\")).groupBy(\"route\").count().orderBy(F.desc(\"count\")).limit(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|        route|count|\n",
      "+-------------+-----+\n",
      "|         main| 8090|\n",
      "| main-archive| 1096|\n",
      "|  main-rabota| 1039|\n",
      "|main-internet|  880|\n",
      "|   main-bonus|  865|\n",
      "+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rts_fin.show(5)\n",
    "# looks fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# uploading data for 30 entries\n",
    "rts_fin.write.csv(\"hdfs:/data/lsml_sga_df_zhukov.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # reading files to check format\n",
    "# se.read.csv(\"hdfs:/data/lsml_sga_sql_zhukov.csv\", header=False, sep=\"\\t\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about complexity - at a first glance we are using only O(n) steps in all 3 solutions."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
